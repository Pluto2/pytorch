{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pluto2/pytorch/blob/main/uformer_dcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkToHMp4Jm5P",
        "outputId": "befa29c1-340e-415d-cc18-507859ba4171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python==3.7\n",
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -U openmim\n",
        "!mim install mmcv-full==1.5.0\n",
        "!pip install timm==0.6.11 mmdet==2.28.1\n",
        "!pip install opencv-python termcolor yacs pyyaml scipy\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcmG5ZyBZDM9",
        "outputId": "f9f2f6c9-46ae-4d32-86de-07dbf761562a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python==3.7 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for python==3.7\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openmim\n",
            "  Downloading openmim-0.3.7-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 KB\u001b[0m \u001b[31m658.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click in /usr/local/lib/python3.9/dist-packages (from openmim) (8.1.3)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.9/dist-packages (from openmim) (22.0.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from openmim) (2.27.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from openmim) (0.8.10)\n",
            "Collecting model-index\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from openmim) (1.4.4)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.9/dist-packages (from model-index->openmim) (3.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from model-index->openmim) (6.0)\n",
            "Collecting ordered-set\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas->openmim) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->openmim) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->openmim) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->openmim) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->openmim) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->openmim) (1.26.15)\n",
            "Collecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m525.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->openmim) (1.16.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown->model-index->openmim) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown->model-index->openmim) (3.15.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m/bin/bash: mim: command not found\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.6.11\n",
            "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmdet==2.28.1\n",
            "  Downloading mmdet-2.28.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm==0.6.11) (6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.6.11) (0.14.1+cu116)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm==0.6.11) (1.13.1+cu116)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mmdet==2.28.1) (3.7.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.9/dist-packages (from mmdet==2.28.1) (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mmdet==2.28.1) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from mmdet==2.28.1) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from mmdet==2.28.1) (1.10.1)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm==0.6.11) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm==0.6.11) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm==0.6.11) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm==0.6.11) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm==0.6.11) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (4.39.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mmdet==2.28.1) (1.0.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mmdet==2.28.1) (3.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm==0.6.11) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm==0.6.11) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm==0.6.11) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm==0.6.11) (1.26.15)\n",
            "Installing collected packages: terminaltables, huggingface-hub, timm, mmdet\n",
            "Successfully installed huggingface-hub-0.13.3 mmdet-2.28.1 terminaltables-3.1.10 timm-0.6.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (2.2.0)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.22.4)\n",
            "Installing collected packages: yacs\n",
            "Successfully installed yacs-0.1.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python==3.7\n",
        "!sh make.sh\n",
        "\n",
        "import DCNv3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "46Mg-vEmIRWe",
        "outputId": "11b9e438-7d4a-45be-9f76-fe3cd941a487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python==3.7 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for python==3.7\u001b[0m\u001b[31m\n",
            "\u001b[0mrunning build\n",
            "running build_ext\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "running install\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing DCNv3.egg-info/PKG-INFO\n",
            "writing dependency_links to DCNv3.egg-info/dependency_links.txt\n",
            "writing top-level names to DCNv3.egg-info/top_level.txt\n",
            "reading manifest file 'DCNv3.egg-info/SOURCES.txt'\n",
            "writing manifest file 'DCNv3.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.9/DCNv3.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for DCNv3.cpython-39-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/DCNv3.py to DCNv3.cpython-39.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying DCNv3.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying DCNv3.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying DCNv3.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying DCNv3.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.DCNv3.cpython-39: module references __file__\n",
            "creating 'dist/DCNv3-1.0-py3.9-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing DCNv3-1.0-py3.9-linux-x86_64.egg\n",
            "removing '/usr/local/lib/python3.9/dist-packages/DCNv3-1.0-py3.9-linux-x86_64.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.9/dist-packages/DCNv3-1.0-py3.9-linux-x86_64.egg\n",
            "Extracting DCNv3-1.0-py3.9-linux-x86_64.egg to /usr/local/lib/python3.9/dist-packages\n",
            "DCNv3 1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Installed /usr/local/lib/python3.9/dist-packages/DCNv3-1.0-py3.9-linux-x86_64.egg\n",
            "Processing dependencies for DCNv3==1.0\n",
            "Finished processing dependencies for DCNv3==1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fa303e04688f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh make.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDCNv3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: dynamic module does not define module export function (PyInit_DCNv3)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-I/usr/include/python3.8 -I/usr/include/python3.8  -Wno-unused-result -Wsign-compare -g -fdebug-prefix-map=/build/python3.8-B32VZ9/python3.8-3.8.10=. -specs=/usr/share/dpkg/no-pie-compile.specs -fstack-protector -Wformat -Werror=format-security  -DNDEBUG -g -fwrapv -O3 -Wall\n",
            "-L/usr/lib/python3.8/config-3.8-x86_64-linux-gnu -L/usr/lib  -lcrypt -lpthread -ldl  -lutil -lm -lm \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a4b2be302e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' python3-config --cflags --ldflags'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mDCNv3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDCNv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: dynamic module does not define module export function (PyInit_DCNv3)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "egg_path='/usr/local/lib/python3.9/dist-packages/DCNv3-1.0-py3.9-linux-x86_64.egg'\n",
        "sys.path.append(egg_path)\n",
        "\n",
        "import DCNv3"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_mgWcCQXIAKW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "25cc5163-7788-40ec-b2db-48df40a8876b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Y83FtvLJIAKS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from torch import einsum\n",
        "# from __future__ import absolute_import\n",
        "# from __future__ import print_function\n",
        "# from __future__ import division\n",
        "\n",
        "import warnings\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.init import xavier_uniform_, constant_\n",
        "# from ..functions import DCNv3Function, dcnv3_core_pytorch\n",
        "from torch.autograd import Function\n",
        "from torch.autograd.function import once_differentiable\n",
        "from torch.cuda.amp import custom_bwd, custom_fwd\n",
        "'''\n",
        "Intern_Image里面的代码\n",
        "---------------Begin-----------------\n",
        "'''\n",
        "class to_channels_first(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "class to_channels_last(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.permute(0, 2, 3, 1)\n",
        "\n",
        "\n",
        "def build_norm_layer(dim,\n",
        "                     norm_layer,\n",
        "                     in_format='channels_last',\n",
        "                     out_format='channels_last',\n",
        "                     eps=1e-6):\n",
        "    layers = []\n",
        "    if norm_layer == 'BN':\n",
        "        if in_format == 'channels_last':\n",
        "            layers.append(to_channels_first())\n",
        "        layers.append(nn.BatchNorm2d(dim))\n",
        "        if out_format == 'channels_last':\n",
        "            layers.append(to_channels_last())\n",
        "    elif norm_layer == 'LN':\n",
        "        if in_format == 'channels_first':\n",
        "            layers.append(to_channels_last())\n",
        "        layers.append(nn.LayerNorm(dim, eps=eps))\n",
        "        if out_format == 'channels_first':\n",
        "            layers.append(to_channels_first())\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f'build_norm_layer does not support {norm_layer}')\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def build_act_layer(act_layer):\n",
        "    if act_layer == 'ReLU':\n",
        "        return nn.ReLU(inplace=True)\n",
        "    elif act_layer == 'SiLU':\n",
        "        return nn.SiLU(inplace=True)\n",
        "    elif act_layer == 'GELU':\n",
        "        return nn.GELU()\n",
        "\n",
        "    raise NotImplementedError(f'build_act_layer does not support {act_layer}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# import DCNv3\n",
        "\n",
        "class DCNv3Function(Function):\n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(\n",
        "            ctx, input, offset, mask,\n",
        "            kernel_h, kernel_w, stride_h, stride_w,\n",
        "            pad_h, pad_w, dilation_h, dilation_w,\n",
        "            group, group_channels, offset_scale, im2col_step):\n",
        "        ctx.kernel_h = kernel_h\n",
        "        ctx.kernel_w = kernel_w\n",
        "        ctx.stride_h = stride_h\n",
        "        ctx.stride_w = stride_w\n",
        "        ctx.pad_h = pad_h\n",
        "        ctx.pad_w = pad_w\n",
        "        ctx.dilation_h = dilation_h\n",
        "        ctx.dilation_w = dilation_w\n",
        "        ctx.group = group\n",
        "        ctx.group_channels = group_channels\n",
        "        ctx.offset_scale = offset_scale\n",
        "        ctx.im2col_step = im2col_step\n",
        "        output = DCNv3.dcnv3_forward(\n",
        "            input, offset, mask, kernel_h,\n",
        "            kernel_w, stride_h, stride_w, pad_h,\n",
        "            pad_w, dilation_h, dilation_w, group,\n",
        "            group_channels, offset_scale, ctx.im2col_step)\n",
        "        ctx.save_for_backward(input, offset, mask)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    @once_differentiable\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output):\n",
        "        input, offset, mask = ctx.saved_tensors\n",
        "        grad_input, grad_offset, grad_mask = \\\n",
        "            DCNv3.dcnv3_backward(\n",
        "                input, offset, mask, ctx.kernel_h,\n",
        "                ctx.kernel_w, ctx.stride_h, ctx.stride_w, ctx.pad_h,\n",
        "                ctx.pad_w, ctx.dilation_h, ctx.dilation_w, ctx.group,\n",
        "                ctx.group_channels, ctx.offset_scale, grad_output.contiguous(), ctx.im2col_step)\n",
        "\n",
        "        return grad_input, grad_offset, grad_mask, \\\n",
        "            None, None, None, None, None, None, None, None, None, None, None, None\n",
        "\n",
        "    @staticmethod\n",
        "    def symbolic(g, input, offset, mask, kernel_h, kernel_w, stride_h,\n",
        "                 stride_w, pad_h, pad_w, dilation_h, dilation_w, group,\n",
        "                 group_channels, offset_scale, im2col_step):\n",
        "        \"\"\"Symbolic function for mmdeploy::DCNv3.\n",
        "\n",
        "        Returns:\n",
        "            DCNv3 op for onnx.\n",
        "        \"\"\"\n",
        "        return g.op(\n",
        "            'mmdeploy::TRTDCNv3',\n",
        "            input,\n",
        "            offset,\n",
        "            mask,\n",
        "            kernel_h_i=int(kernel_h),\n",
        "            kernel_w_i=int(kernel_w),\n",
        "            stride_h_i=int(stride_h),\n",
        "            stride_w_i=int(stride_w),\n",
        "            pad_h_i=int(pad_h),\n",
        "            pad_w_i=int(pad_w),\n",
        "            dilation_h_i=int(dilation_h),\n",
        "            dilation_w_i=int(dilation_w),\n",
        "            group_i=int(group),\n",
        "            group_channels_i=int(group_channels),\n",
        "            offset_scale_f=float(offset_scale),\n",
        "            im2col_step_i=int(im2col_step),\n",
        "        )\n",
        "\n",
        "\n",
        "def _get_reference_points(spatial_shapes, device, kernel_h, kernel_w, dilation_h, dilation_w, pad_h=0, pad_w=0, stride_h=1, stride_w=1):\n",
        "    _, H_, W_, _ = spatial_shapes\n",
        "    H_out = (H_ - (dilation_h * (kernel_h - 1) + 1)) // stride_h + 1\n",
        "    W_out = (W_ - (dilation_w * (kernel_w - 1) + 1)) // stride_w + 1\n",
        "\n",
        "    ref_y, ref_x = torch.meshgrid(\n",
        "        torch.linspace(\n",
        "            # pad_h + 0.5,\n",
        "            # H_ - pad_h - 0.5,\n",
        "            (dilation_h * (kernel_h - 1)) // 2 + 0.5,\n",
        "            (dilation_h * (kernel_h - 1)) // 2 + 0.5 + (H_out - 1) * stride_h,\n",
        "            H_out,\n",
        "            dtype=torch.float32,\n",
        "            device=device),\n",
        "        torch.linspace(\n",
        "            # pad_w + 0.5,\n",
        "            # W_ - pad_w - 0.5,\n",
        "            (dilation_w * (kernel_w - 1)) // 2 + 0.5,\n",
        "            (dilation_w * (kernel_w - 1)) // 2 + 0.5 + (W_out - 1) * stride_w,\n",
        "            W_out,\n",
        "            dtype=torch.float32,\n",
        "            device=device))\n",
        "    ref_y = ref_y.reshape(-1)[None] / H_\n",
        "    ref_x = ref_x.reshape(-1)[None] / W_\n",
        "\n",
        "    ref = torch.stack((ref_x, ref_y), -1).reshape(\n",
        "        1, H_out, W_out, 1, 2)\n",
        "\n",
        "    return ref\n",
        "\n",
        "\n",
        "def _generate_dilation_grids(spatial_shapes, kernel_h, kernel_w, dilation_h, dilation_w, group, device):\n",
        "    _, H_, W_, _ = spatial_shapes\n",
        "    points_list = []\n",
        "    x, y = torch.meshgrid(\n",
        "        torch.linspace(\n",
        "            -((dilation_w * (kernel_w - 1)) // 2),\n",
        "            -((dilation_w * (kernel_w - 1)) // 2) +\n",
        "            (kernel_w - 1) * dilation_w, kernel_w,\n",
        "            dtype=torch.float32,\n",
        "            device=device),\n",
        "        torch.linspace(\n",
        "            -((dilation_h * (kernel_h - 1)) // 2),\n",
        "            -((dilation_h * (kernel_h - 1)) // 2) +\n",
        "            (kernel_h - 1) * dilation_h, kernel_h,\n",
        "            dtype=torch.float32,\n",
        "            device=device))\n",
        "\n",
        "    points_list.extend([x / W_, y / H_])\n",
        "    grid = torch.stack(points_list, -1).reshape(-1, 1, 2).\\\n",
        "        repeat(1, group, 1).permute(1, 0, 2)\n",
        "    grid = grid.reshape(1, 1, 1, group * kernel_h * kernel_w, 2)\n",
        "\n",
        "    return grid\n",
        "\n",
        "\n",
        "def dcnv3_core_pytorch(\n",
        "        input, offset, mask, kernel_h,\n",
        "        kernel_w, stride_h, stride_w, pad_h,\n",
        "        pad_w, dilation_h, dilation_w, group,\n",
        "        group_channels, offset_scale):\n",
        "    # for debug and test only,\n",
        "    # need to use cuda version instead\n",
        "    input = F.pad(\n",
        "        input,\n",
        "        [0, 0, pad_h, pad_h, pad_w, pad_w])\n",
        "    N_, H_in, W_in, _ = input.shape\n",
        "    _, H_out, W_out, _ = offset.shape\n",
        "\n",
        "    ref = _get_reference_points(\n",
        "        input.shape, input.device, kernel_h, kernel_w, dilation_h, dilation_w, pad_h, pad_w, stride_h, stride_w)\n",
        "    grid = _generate_dilation_grids(\n",
        "        input.shape, kernel_h, kernel_w, dilation_h, dilation_w, group, input.device)\n",
        "    spatial_norm = torch.tensor([W_in, H_in]).reshape(1, 1, 1, 2).\\\n",
        "        repeat(1, 1, 1, group*kernel_h*kernel_w).to(input.device)\n",
        "\n",
        "    sampling_locations = (ref + grid * offset_scale).repeat(N_, 1, 1, 1, 1).flatten(3, 4) + \\\n",
        "        offset * offset_scale / spatial_norm\n",
        "\n",
        "    P_ = kernel_h * kernel_w\n",
        "    sampling_grids = 2 * sampling_locations - 1\n",
        "    # N_, H_in, W_in, group*group_channels -> N_, H_in*W_in, group*group_channels -> N_, group*group_channels, H_in*W_in -> N_*group, group_channels, H_in, W_in\n",
        "    input_ = input.view(N_, H_in*W_in, group*group_channels).transpose(1, 2).\\\n",
        "        reshape(N_*group, group_channels, H_in, W_in)\n",
        "    # N_, H_out, W_out, group*P_*2 -> N_, H_out*W_out, group, P_, 2 -> N_, group, H_out*W_out, P_, 2 -> N_*group, H_out*W_out, P_, 2\n",
        "    sampling_grid_ = sampling_grids.view(N_, H_out*W_out, group, P_, 2).transpose(1, 2).\\\n",
        "        flatten(0, 1)\n",
        "    # N_*group, group_channels, H_out*W_out, P_\n",
        "    sampling_input_ = F.grid_sample(\n",
        "        input_, sampling_grid_, mode='bilinear', padding_mode='zeros', align_corners=False)\n",
        "\n",
        "    # (N_, H_out, W_out, group*P_) -> N_, H_out*W_out, group, P_ -> (N_, group, H_out*W_out, P_) -> (N_*group, 1, H_out*W_out, P_)\n",
        "    mask = mask.view(N_, H_out*W_out, group, P_).transpose(1, 2).\\\n",
        "        reshape(N_*group, 1, H_out*W_out, P_)\n",
        "    output = (sampling_input_ * mask).sum(-1).view(N_,\n",
        "                                                   group*group_channels, H_out*W_out)\n",
        "\n",
        "    return output.transpose(1, 2).reshape(N_, H_out, W_out, -1).contiguous()\n",
        "\n",
        "\"\"\"\n",
        "代替 from ops_dcnv3 import modules as opsm\n",
        "这里直接复制 ops_dcnv3/modules\n",
        "\"\"\"\n",
        "# --------------------------------------------------------\n",
        "# InternImage\n",
        "# Copyright (c) 2022 OpenGVLab\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# --------------------------------------------------------\n",
        "def _is_power_of_2(n):\n",
        "    if (not isinstance(n, int)) or (n < 0):\n",
        "        raise ValueError(\n",
        "            \"invalid input for _is_power_of_2: {} (type: {})\".format(n, type(n)))\n",
        "\n",
        "    return (n & (n - 1) == 0) and n != 0\n",
        "\n",
        "\n",
        "class CenterFeatureScaleModule(nn.Module):\n",
        "    def forward(self,\n",
        "                query,\n",
        "                center_feature_scale_proj_weight,\n",
        "                center_feature_scale_proj_bias):\n",
        "        center_feature_scale = F.linear(query,\n",
        "                                        weight=center_feature_scale_proj_weight,\n",
        "                                        bias=center_feature_scale_proj_bias).sigmoid()\n",
        "        return center_feature_scale\n",
        "\n",
        "\n",
        "class DCNv3_pytorch(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            channels=64,\n",
        "            kernel_size=3,\n",
        "            dw_kernel_size=None,\n",
        "            stride=1,\n",
        "            pad=1,\n",
        "            dilation=1,\n",
        "            group=4,\n",
        "            offset_scale=1.0,\n",
        "            act_layer='GELU',\n",
        "            norm_layer='LN',\n",
        "            center_feature_scale=False):\n",
        "        \"\"\"\n",
        "        DCNv3 Module\n",
        "        :param channels\n",
        "        :param kernel_size\n",
        "        :param stride\n",
        "        :param pad\n",
        "        :param dilation\n",
        "        :param group\n",
        "        :param offset_scale\n",
        "        :param act_layer\n",
        "        :param norm_layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if channels % group != 0:\n",
        "            raise ValueError(\n",
        "                f'channels must be divisible by group, but got {channels} and {group}')\n",
        "        _d_per_group = channels // group\n",
        "        dw_kernel_size = dw_kernel_size if dw_kernel_size is not None else kernel_size\n",
        "        # you'd better set _d_per_group to a power of 2 which is more efficient in our CUDA implementation\n",
        "        if not _is_power_of_2(_d_per_group):\n",
        "            warnings.warn(\n",
        "                \"You'd better set channels in DCNv3 to make the dimension of each attention head a power of 2 \"\n",
        "                \"which is more efficient in our CUDA implementation.\")\n",
        "\n",
        "        self.offset_scale = offset_scale\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dw_kernel_size = dw_kernel_size\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.pad = pad\n",
        "        self.group = group\n",
        "        self.group_channels = channels // group\n",
        "        self.offset_scale = offset_scale\n",
        "        self.center_feature_scale = center_feature_scale\n",
        "\n",
        "        self.dw_conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                channels,\n",
        "                channels,\n",
        "                kernel_size=dw_kernel_size,\n",
        "                stride=1,\n",
        "                padding=(dw_kernel_size - 1) // 2,\n",
        "                groups=channels),\n",
        "            build_norm_layer(\n",
        "                channels,\n",
        "                norm_layer,\n",
        "                'channels_first',\n",
        "                'channels_last'),\n",
        "            build_act_layer(act_layer))\n",
        "        self.offset = nn.Linear(\n",
        "            channels,\n",
        "            group * kernel_size * kernel_size * 2)\n",
        "        self.mask = nn.Linear(\n",
        "            channels,\n",
        "            group * kernel_size * kernel_size)\n",
        "        self.input_proj = nn.Linear(channels, channels)\n",
        "        self.output_proj = nn.Linear(channels, channels)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        if center_feature_scale:\n",
        "            self.center_feature_scale_proj_weight = nn.Parameter(\n",
        "                torch.zeros((group, channels), dtype=torch.float))\n",
        "            self.center_feature_scale_proj_bias = nn.Parameter(\n",
        "                torch.tensor(0.0, dtype=torch.float).view((1,)).repeat(group, ))\n",
        "            self.center_feature_scale_module = CenterFeatureScaleModule()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        constant_(self.offset.weight.data, 0.)\n",
        "        constant_(self.offset.bias.data, 0.)\n",
        "        constant_(self.mask.weight.data, 0.)\n",
        "        constant_(self.mask.bias.data, 0.)\n",
        "        xavier_uniform_(self.input_proj.weight.data)\n",
        "        constant_(self.input_proj.bias.data, 0.)\n",
        "        xavier_uniform_(self.output_proj.weight.data)\n",
        "        constant_(self.output_proj.bias.data, 0.)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param query                       (N, H, W, C)\n",
        "        :return output                     (N, H, W, C)\n",
        "        \"\"\"\n",
        "        N, H, W, _ = input.shape\n",
        "\n",
        "        x = self.input_proj(input)\n",
        "        x_proj = x\n",
        "\n",
        "        x1 = input.permute(0, 3, 1, 2)\n",
        "        x1 = self.dw_conv(x1)\n",
        "        offset = self.offset(x1)\n",
        "        mask = self.mask(x1).reshape(N, H, W, self.group, -1)\n",
        "        mask = F.softmax(mask, -1).reshape(N, H, W, -1)\n",
        "\n",
        "        x = dcnv3_core_pytorch(\n",
        "            x, offset, mask,\n",
        "            self.kernel_size, self.kernel_size,\n",
        "            self.stride, self.stride,\n",
        "            self.pad, self.pad,\n",
        "            self.dilation, self.dilation,\n",
        "            self.group, self.group_channels,\n",
        "            self.offset_scale)\n",
        "        if self.center_feature_scale:\n",
        "            center_feature_scale = self.center_feature_scale_module(\n",
        "                x1, self.center_feature_scale_proj_weight, self.center_feature_scale_proj_bias)\n",
        "            # N, H, W, groups -> N, H, W, groups, 1 -> N, H, W, groups, _d_per_group -> N, H, W, channels\n",
        "            center_feature_scale = center_feature_scale[..., None].repeat(\n",
        "                1, 1, 1, 1, self.channels // self.group).flatten(-2)\n",
        "            x = x * (1 - center_feature_scale) + x_proj * center_feature_scale\n",
        "        x = self.output_proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCNv3(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            channels=64,\n",
        "            kernel_size=3,\n",
        "            dw_kernel_size=None,\n",
        "            stride=1,\n",
        "            pad=1,\n",
        "            dilation=1,\n",
        "            group=4,\n",
        "            offset_scale=1.0,\n",
        "            act_layer='GELU',\n",
        "            norm_layer='LN',\n",
        "            center_feature_scale=False):\n",
        "        \"\"\"\n",
        "        DCNv3 Module\n",
        "        :param channels\n",
        "        :param kernel_size\n",
        "        :param stride\n",
        "        :param pad\n",
        "        :param dilation\n",
        "        :param group\n",
        "        :param offset_scale\n",
        "        :param act_layer\n",
        "        :param norm_layer\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if channels % group != 0:\n",
        "            raise ValueError(\n",
        "                f'channels must be divisible by group, but got {channels} and {group}')\n",
        "        _d_per_group = channels // group\n",
        "        dw_kernel_size = dw_kernel_size if dw_kernel_size is not None else kernel_size\n",
        "        # you'd better set _d_per_group to a power of 2 which is more efficient in our CUDA implementation\n",
        "        if not _is_power_of_2(_d_per_group):\n",
        "            warnings.warn(\n",
        "                \"You'd better set channels in DCNv3 to make the dimension of each attention head a power of 2 \"\n",
        "                \"which is more efficient in our CUDA implementation.\")\n",
        "\n",
        "        self.offset_scale = offset_scale\n",
        "        self.channels = channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dw_kernel_size = dw_kernel_size\n",
        "        self.stride = stride\n",
        "        self.dilation = dilation\n",
        "        self.pad = pad\n",
        "        self.group = group\n",
        "        self.group_channels = channels // group\n",
        "        self.offset_scale = offset_scale\n",
        "        self.center_feature_scale = center_feature_scale\n",
        "\n",
        "        self.dw_conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                channels,\n",
        "                channels,\n",
        "                kernel_size=dw_kernel_size,\n",
        "                stride=1,\n",
        "                padding=(dw_kernel_size - 1) // 2,\n",
        "                groups=channels),\n",
        "            build_norm_layer(\n",
        "                channels,\n",
        "                norm_layer,\n",
        "                'channels_first',\n",
        "                'channels_last'),\n",
        "            build_act_layer(act_layer))\n",
        "        self.offset = nn.Linear(\n",
        "            channels,\n",
        "            group * kernel_size * kernel_size * 2)\n",
        "        self.mask = nn.Linear(\n",
        "            channels,\n",
        "            group * kernel_size * kernel_size)\n",
        "        self.input_proj = nn.Linear(channels, channels)\n",
        "        self.output_proj = nn.Linear(channels, channels)\n",
        "        self._reset_parameters()\n",
        "\n",
        "        if center_feature_scale:\n",
        "            self.center_feature_scale_proj_weight = nn.Parameter(\n",
        "                torch.zeros((group, channels), dtype=torch.float))\n",
        "            self.center_feature_scale_proj_bias = nn.Parameter(\n",
        "                torch.tensor(0.0, dtype=torch.float).view((1,)).repeat(group, ))\n",
        "            self.center_feature_scale_module = CenterFeatureScaleModule()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        constant_(self.offset.weight.data, 0.)\n",
        "        constant_(self.offset.bias.data, 0.)\n",
        "        constant_(self.mask.weight.data, 0.)\n",
        "        constant_(self.mask.bias.data, 0.)\n",
        "        xavier_uniform_(self.input_proj.weight.data)\n",
        "        constant_(self.input_proj.bias.data, 0.)\n",
        "        xavier_uniform_(self.output_proj.weight.data)\n",
        "        constant_(self.output_proj.bias.data, 0.)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param query                       (N, H, W, C)\n",
        "        :return output                     (N, H, W, C)\n",
        "        \"\"\"\n",
        "        N, H, W, _ = input.shape\n",
        "\n",
        "        x = self.input_proj(input)\n",
        "        x_proj = x\n",
        "        dtype = x.dtype\n",
        "\n",
        "        x1 = input.permute(0, 3, 1, 2)\n",
        "        x1 = self.dw_conv(x1)\n",
        "        offset = self.offset(x1)\n",
        "        mask = self.mask(x1).reshape(N, H, W, self.group, -1)\n",
        "        mask = F.softmax(mask, -1).reshape(N, H, W, -1).type(dtype)\n",
        "\n",
        "        x = DCNv3Function.apply(\n",
        "            x, offset, mask,\n",
        "            self.kernel_size, self.kernel_size,\n",
        "            self.stride, self.stride,\n",
        "            self.pad, self.pad,\n",
        "            self.dilation, self.dilation,\n",
        "            self.group, self.group_channels,\n",
        "            self.offset_scale,\n",
        "            256)\n",
        "\n",
        "        if self.center_feature_scale:\n",
        "            center_feature_scale = self.center_feature_scale_module(\n",
        "                x1, self.center_feature_scale_proj_weight, self.center_feature_scale_proj_bias)\n",
        "            # N, H, W, groups -> N, H, W, groups, 1 -> N, H, W, groups, _d_per_group -> N, H, W, channels\n",
        "            center_feature_scale = center_feature_scale[..., None].repeat(\n",
        "                1, 1, 1, 1, self.channels // self.group).flatten(-2)\n",
        "            x = x * (1 - center_feature_scale) + x_proj * center_feature_scale\n",
        "        x = self.output_proj(x)\n",
        "\n",
        "        return x\n",
        "# --------------------------end----只是DCNv3模块---------------\n",
        "\n",
        "\n",
        "class MLPLayer(nn.Module):\n",
        "    r\"\"\" MLP layer of InternImage\n",
        "    Args:\n",
        "        in_features (int): number of input features\n",
        "        hidden_features (int): number of hidden features\n",
        "        out_features (int): number of output features\n",
        "        act_layer (str): activation layer\n",
        "        drop (float): dropout rate\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features=None,\n",
        "                 out_features=None,\n",
        "                 act_layer='GELU',\n",
        "                 drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = build_act_layer(act_layer)\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "## H×W×3 ---》 H×W×C\n",
        "## 作为浅特征提取模块\n",
        "class StemLayer(nn.Module):\n",
        "    r\"\"\" Stem layer of InternImage\n",
        "    Args:\n",
        "        in_chans (int): number of input channels\n",
        "        out_chans (int): number of output channels\n",
        "        act_layer (str): activation layer\n",
        "        norm_layer (str): normalization layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_chans=3,\n",
        "                 out_chans=32,\n",
        "                 act_layer='GELU',\n",
        "                 norm_layer='BN'):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_chans,\n",
        "                               out_chans // 2,\n",
        "                               kernel_size=3,\n",
        "                               stride=1,\n",
        "                               padding=1)\n",
        "        self.norm1 = build_norm_layer(out_chans // 2, norm_layer,\n",
        "                                      'channels_first', 'channels_first')\n",
        "        self.act = build_act_layer(act_layer)\n",
        "        self.conv2 = nn.Conv2d(out_chans // 2,\n",
        "                               out_chans,\n",
        "                               kernel_size=3,\n",
        "                               stride=1,\n",
        "                               padding=1)\n",
        "        self.norm2 = build_norm_layer(out_chans, norm_layer, 'channels_first',\n",
        "                                      'channels_last')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "F9i1bZ2MIAKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Output Projection\n",
        "# B, C, H, W\n",
        "class OutputProj(nn.Module):\n",
        "    def __init__(self, in_channel=64, out_channel=3, kernel_size=3, stride=1, norm_layer=None,act_layer=None):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=kernel_size//2),\n",
        "        )\n",
        "        if act_layer is not None:\n",
        "            self.proj.add_module(act_layer(inplace=True))\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(out_channel)\n",
        "        else:\n",
        "            self.norm = None\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0,3,1,2)\n",
        "        x = self.proj(x)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        flops = 0\n",
        "        # conv\n",
        "        flops += H*W*self.in_channel*self.out_channel*3*3\n",
        "\n",
        "        if self.norm is not None:\n",
        "            flops += H*W*self.out_channel\n",
        "        print(\"Output_proj:{%.2f}\"%(flops/1e9))\n",
        "        return flops"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AzZvHSn5IAKc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "################ InternImage的downsample/upsample #########################\n",
        "class DownsampleLayer(nn.Module):\n",
        "    r\"\"\" Downsample layer of InternImage\n",
        "    Args:\n",
        "        channels (int): number of input channels\n",
        "        norm_layer (str): normalization layer\n",
        "    \"\"\"\n",
        "    def __init__(self, channels, norm_layer='LN'):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels,\n",
        "                              2 * channels,\n",
        "                              kernel_size=3,\n",
        "                              stride=2,\n",
        "                              padding=1,\n",
        "                              bias=False)\n",
        "        self.norm = build_norm_layer(2 * channels, norm_layer,\n",
        "                                     'channels_first', 'channels_last')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x.permute(0, 3, 1, 2)) # channels_first\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "##\n",
        "class UpsampleLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channel, norm_layer='LN'):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(in_channels,\n",
        "                              out_channel,\n",
        "                              kernel_size=3,\n",
        "                              stride=2,\n",
        "                              padding=1,\n",
        "                              output_padding=1,\n",
        "                              bias=False)\n",
        "        self.norm = build_norm_layer(out_channel, norm_layer,\n",
        "                                     'channels_first', 'channels_last')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x.permute(0, 3, 1, 2)) # first_channel\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x\n",
        "# downsample = DownsampleLayer(32)\n",
        "# upsample = UpsampleLayer(64)\n",
        "# img_down = downsample(x)\n",
        "# img_up = upsample(img_down)\n",
        "# img_down.shape,img_up.shape,x.shape"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XyKFPb1-IAKb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28, 32])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## 模仿transformer架构，DCNv3替换注意力模块\n",
        "class DCNv3Layer(nn.Module):\n",
        "    r\"\"\" Basic layer of InternImage\n",
        "    Args:\n",
        "        core_op (nn.Module): core operation of InternImage\n",
        "        channels (int): number of input channels\n",
        "        groups (list): Groups of each block.\n",
        "        mlp_ratio (float): ratio of mlp hidden features to input channels\n",
        "        drop (float): dropout rate\n",
        "        drop_path (float): drop path rate\n",
        "        act_layer (str): activation layer\n",
        "        norm_layer (str): normalization layer\n",
        "        post_norm (bool): whether to use post normalization\n",
        "        layer_scale (float): layer scale\n",
        "        offset_scale (float): offset scale\n",
        "        with_cp (bool): whether to use checkpoint\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 groups,\n",
        "                 mlp_ratio=4.,\n",
        "                 drop=0.,\n",
        "                 drop_path=0.,\n",
        "                 act_layer='GELU',\n",
        "                 norm_layer='LN',\n",
        "                 post_norm=False,\n",
        "                 layer_scale=None,\n",
        "                 offset_scale=1.0,\n",
        "                 with_cp=False):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.groups = groups\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.with_cp = with_cp\n",
        "\n",
        "        self.norm1 = build_norm_layer(channels, 'LN')\n",
        "        self.post_norm = post_norm\n",
        "        self.dcn = DCNv3_pytorch(channels=channels,\n",
        "                           kernel_size=3,\n",
        "                           stride=1,\n",
        "                           pad=1,\n",
        "                           dilation=1,\n",
        "                           group=groups,\n",
        "                           offset_scale=offset_scale,\n",
        "                           act_layer=act_layer,\n",
        "                           norm_layer=norm_layer)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. \\\n",
        "            else nn.Identity()\n",
        "        self.norm2 = build_norm_layer(channels, 'LN')\n",
        "        self.mlp = MLPLayer(in_features=channels,\n",
        "                            hidden_features=int(channels * mlp_ratio),\n",
        "                            act_layer=act_layer,\n",
        "                            drop=drop)\n",
        "        self.layer_scale = layer_scale is not None\n",
        "        if self.layer_scale:\n",
        "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(channels),\n",
        "                                       requires_grad=True)\n",
        "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(channels),\n",
        "                                       requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        def _inner_forward(x):\n",
        "            if not self.layer_scale:\n",
        "                if self.post_norm:\n",
        "                    x = x + self.drop_path(self.norm1(self.dcn(x)))\n",
        "                    x = x + self.drop_path(self.norm2(self.mlp(x)))\n",
        "                else:\n",
        "                    x = x + self.drop_path(self.dcn(self.norm1(x)))\n",
        "                    x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "                return x\n",
        "            if self.post_norm:\n",
        "                x = x + self.drop_path(self.gamma1 * self.norm1(self.dcn(x)))\n",
        "                x = x + self.drop_path(self.gamma2 * self.norm2(self.mlp(x)))\n",
        "            else:\n",
        "                x = x + self.drop_path(self.gamma1 * self.dcn(self.norm1(x)))\n",
        "                x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))\n",
        "            return x\n",
        "\n",
        "        if self.with_cp and x.requires_grad:\n",
        "            x = checkpoint.checkpoint(_inner_forward, x)\n",
        "        else:\n",
        "            x = _inner_forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCNv3Block(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 depth,\n",
        "                 groups,\n",
        "                 mlp_ratio=4.,\n",
        "                 drop=0.,\n",
        "                 drop_path=0.,\n",
        "                 act_layer='GELU',\n",
        "                 norm_layer='LN',\n",
        "                 post_norm=False,\n",
        "                 offset_scale=1.0,\n",
        "                 layer_scale=None,\n",
        "                 with_cp=False):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.depth = depth \n",
        "        self.post_norm = post_norm\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DCNv3Layer(channels=channels,\n",
        "                             groups=groups,\n",
        "                             mlp_ratio=mlp_ratio,\n",
        "                             drop=drop,\n",
        "                             drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                             act_layer=act_layer,\n",
        "                             norm_layer=norm_layer,\n",
        "                             post_norm=post_norm,\n",
        "                             layer_scale=layer_scale,\n",
        "                             offset_scale=offset_scale,\n",
        "                             with_cp=with_cp) for i in range(depth)\n",
        "        ])\n",
        "        if not self.post_norm:\n",
        "            self.norm = build_norm_layer(channels, 'LN')\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x:\n",
        "        :param return_wo_downsample: 这个参数的作用是为了在需要对模型的中间结果进行分析或可视化时，方便地获取模型的所有层的输出。\n",
        "        :return:\n",
        "        '''\n",
        "        # 将输入张量 x 通过 self.blocks 中的所有块进行前向传递，得到输出张量 x\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        if not self.post_norm:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "----------------end------------------\n",
        "'''\n",
        "\n",
        "x = torch.randn(1, 28, 28, 3).permute(0, 3, 1, 2)\n",
        "embed_dim = 32\n",
        "mlp_ratio=4.\n",
        "drop_rate=0.\n",
        "drop_path_rate=0.1\n",
        "depths=[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
        "num_enc_layers = len(depths)//2\n",
        "num_dec_layers = len(depths)//2\n",
        "enc_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths[:num_enc_layers]))]\n",
        "conv_dpr = [drop_path_rate]*depths[4]\n",
        "dec_dpr = enc_dpr[::-1]\n",
        "input = StemLayer(3, embed_dim, 'GELU', 'BN')\n",
        "layer = DCNv3Layer(\n",
        "    embed_dim, 2, mlp_ratio, drop_rate, act_layer='GELU',\n",
        "            norm_layer='LN',\n",
        "            post_norm=False,\n",
        "            layer_scale=None,\n",
        "            offset_scale=1.5,\n",
        "            with_cp=False\n",
        ")\n",
        "block = DCNv3Block(channels=embed_dim,\n",
        "            depth=2,\n",
        "            groups=2,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            drop=drop_rate,  # drop = []\n",
        "            drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "            act_layer='GELU',\n",
        "            norm_layer='LN',\n",
        "            post_norm=False,\n",
        "            layer_scale=None,\n",
        "            offset_scale=1.5,\n",
        "            with_cp=False)\n",
        "x = input(x)\n",
        "x.shape\n",
        "x = layer(x)\n",
        "x = layer(x)\n",
        "x.shape\n",
        "print(block(x).shape)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yPfJD9pkIAKa",
        "outputId": "e8b3fd36-4326-4d90-d9a8-65b9129cfdf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 256, 256, 16])\n",
            "torch.Size([1, 256, 256, 32])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "class Uformer(nn.Module):\n",
        "    r'''\n",
        "        channels,\n",
        "        depth,\n",
        "        groups,\n",
        "        downsample=True,\n",
        "        mlp_ratio=4.,\n",
        "        drop=0.,\n",
        "        drop_path=0.,\n",
        "        act_layer='GELU',\n",
        "        norm_layer='LN',\n",
        "        post_norm=False,\n",
        "        offset_scale=1.0,\n",
        "        layer_scale=None,\n",
        "        with_cp=False\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 # img_size=256,\n",
        "                 in_chans=3,\n",
        "                 dd_in=3,\n",
        "                 embed_dim=32,\n",
        "                 depths=[2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
        "                 # num_heads=[1, 2, 4, 8, 16, 16, 8, 4, 2], # groups\n",
        "                 # win_size=8,\n",
        "                 mlp_ratio=4.,\n",
        "                 # qkv_bias=True,\n",
        "                 # qk_scale=None,\n",
        "                 drop_rate=0.,\n",
        "                 # attn_drop_rate=0.,\n",
        "                 drop_path_rate=0.1,\n",
        "                 # norm_layer=nn.LayerNorm,\n",
        "                 patch_norm=True,\n",
        "                 use_checkpoint=False,\n",
        "                 token_projection='linear',\n",
        "                 token_mlp='leff',\n",
        "                 dowsample=DownsampleLayer,\n",
        "                 upsample=UpsampleLayer,\n",
        "                 # shift_flag=True,\n",
        "                 # modulator=False,\n",
        "                 # cross_modulator=False, **kwargs\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_enc_layers = len(depths)//2\n",
        "        self.num_dec_layers = len(depths)//2\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.token_projection = token_projection\n",
        "        self.mlp = token_mlp\n",
        "        # self.win_size =win_size\n",
        "        # self.reso = img_size\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "        self.dd_in = dd_in\n",
        "\n",
        "        # stochastic depth\n",
        "        # dropout率\n",
        "        enc_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths[:self.num_enc_layers]))]\n",
        "        conv_dpr = [drop_path_rate]*depths[4]\n",
        "        dec_dpr = enc_dpr[::-1]\n",
        "\n",
        "        # build layers\n",
        "\n",
        "        # Input/Output\n",
        "        self.input_proj = StemLayer(3, embed_dim, 'GELU', 'BN') ## B, C, H, W\n",
        "        self.output_proj = OutputProj(in_channel=2*embed_dim, out_channel=in_chans, kernel_size=3, stride=1)\n",
        "\n",
        "        # Encoder\n",
        "        self.en_layer0 = DCNv3Block(channels=embed_dim,\n",
        "                             depth=depths[0],\n",
        "                             groups=2,\n",
        "                             mlp_ratio=mlp_ratio,\n",
        "                             drop=drop_rate,  # drop = []\n",
        "                             drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                             act_layer='GELU',\n",
        "                             norm_layer='LN',\n",
        "                             post_norm=False,\n",
        "                             layer_scale=None,\n",
        "                             offset_scale=1.5,\n",
        "                             with_cp=False)\n",
        "        self.dowsample_0 = dowsample(embed_dim)\n",
        "        self.en_layer1 = DCNv3Block(channels=embed_dim*2,\n",
        "                                    depth=depths[1],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "        self.dowsample_1 = dowsample(embed_dim*2)\n",
        "        self.en_layer2 = DCNv3Block(channels=embed_dim * 4,\n",
        "                                    depth=depths[2],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "        self.dowsample_2 = dowsample(embed_dim*4)\n",
        "        self.en_layer3 = DCNv3Block(channels=embed_dim * 8,\n",
        "                                    depth=depths[3],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "        self.dowsample_3 = dowsample(embed_dim*8)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv = DCNv3Block(channels=embed_dim * 16,\n",
        "                                    depth=depths[4],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample_0 = upsample(embed_dim*16, embed_dim*8)\n",
        "        self.de_layer0 = DCNv3Block(channels=embed_dim * 16,\n",
        "                                    depth=depths[5],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "        self.upsample_1 = upsample(embed_dim*16, embed_dim*4)\n",
        "        self.de_layer1 = DCNv3Block(channels=embed_dim * 8,\n",
        "                                    depth=depths[6],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "        self.upsample_2 = upsample(embed_dim*8, embed_dim*2)\n",
        "        self.de_layer2 = DCNv3Block(channels=embed_dim * 4,\n",
        "                                    depth=depths[7],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "        self.upsample_3 = upsample(embed_dim*4, embed_dim)\n",
        "        self.de_layer3 = DCNv3Block(channels=embed_dim * 2,\n",
        "                                    depth=depths[8],\n",
        "                                    groups=2,\n",
        "                                    mlp_ratio=mlp_ratio,\n",
        "                                    drop=drop_rate,  # drop = []\n",
        "                                    drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                                    act_layer='GELU',\n",
        "                                    norm_layer='LN',\n",
        "                                    post_norm=False,\n",
        "                                    layer_scale=None,\n",
        "                                    offset_scale=1.5,\n",
        "                                    with_cp=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"embed_dim={self.embed_dim}, token_projection={self.token_projection}, token_mlp={self.mlp}\"\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Input Projection\n",
        "        y = self.input_proj(x)\n",
        "        y = self.pos_drop(y)\n",
        "        #Encoder\n",
        "        # conv0 = self.encoderlayer_0(y,mask=mask)\n",
        "        print(y.shape)\n",
        "        conv0 = self.en_layer0(y)\n",
        "        # print(conv0.shape)\n",
        "        pool0 = self.dowsample_0(conv0)\n",
        "        # print(pool0.shape)\n",
        "        # conv1 = self.encoderlayer_1(pool0,mask=mask)\n",
        "        conv1 = self.en_layer1(pool0)\n",
        "        pool1 = self.dowsample_1(conv1)\n",
        "        # print(pool1.shape)\n",
        "        # conv2 = self.encoderlayer_2(pool1,mask=mask)\n",
        "        conv2 = self.en_layer2(pool1)\n",
        "        pool2 = self.dowsample_2(conv2)\n",
        "        # print(pool2.shape)\n",
        "        # conv3 = self.encoderlayer_3(pool2,mask=mask)\n",
        "        conv3 = self.en_layer3(pool2)\n",
        "        # print(conv3.shape)\n",
        "        pool3 = self.dowsample_3(conv3)\n",
        "        # print(pool3.shape)\n",
        "\n",
        "        # Bottleneck\n",
        "        # conv4 = self.conv(pool3, mask=mask)\n",
        "        conv4 = self.conv(pool3)\n",
        "\n",
        "        #Decoder\n",
        "        up0 = self.upsample_0(conv4)\n",
        "        # print(up0.shape)\n",
        "\n",
        "        # skip connection\n",
        "        deconv0 = torch.cat([up0,conv3],-1)\n",
        "        # print(deconv0.shape)\n",
        "\n",
        "        deconv0 = self.de_layer0(deconv0)\n",
        "        # print(deconv0.shape)\n",
        "\n",
        "        up1 = self.upsample_1(deconv0)\n",
        "        # print(up1.shape)\n",
        "        deconv1 = torch.cat([up1,conv2],-1)\n",
        "        # deconv1 = self.decoderlayer_1(deconv1,mask=mask)\n",
        "        # print(deconv1.shape)\n",
        "        deconv1 = self.de_layer1(deconv1)\n",
        "\n",
        "        up2 = self.upsample_2(deconv1)\n",
        "        deconv2 = torch.cat([up2,conv1],-1)\n",
        "        # deconv2 = self.decoderlayer_2(deconv2,mask=mask)\n",
        "        deconv2 = self.de_layer2(deconv2)\n",
        "\n",
        "        up3 = self.upsample_3(deconv2)\n",
        "        deconv3 = torch.cat([up3,conv0],-1)\n",
        "        # deconv3 = self.decoderlayer_3(deconv3,mask=mask)\n",
        "        deconv3 = self.de_layer3(deconv3)\n",
        "        print(deconv3.shape)\n",
        "\n",
        "        # Output Projection\n",
        "        y = self.output_proj(deconv3)\n",
        "        return x + y if self.dd_in ==3 else y\n",
        "\n",
        "    # def flops(self):\n",
        "    #     flops = 0\n",
        "    #     # Input Projection\n",
        "    #     flops += self.input_proj.flops(self.reso,self.reso)\n",
        "    #     # Encoder\n",
        "    #     flops += self.en_layer0.flops()+self.dowsample_0.flops(self.reso,self.reso)\n",
        "    #     flops += self.en_layer1.flops()+self.dowsample_1.flops(self.reso//2,self.reso//2)\n",
        "    #     flops += self.en_layer2.flops()+self.dowsample_2.flops(self.reso//2**2,self.reso//2**2)\n",
        "    #     flops += self.en_layer3.flops()+self.dowsample_3.flops(self.reso//2**3,self.reso//2**3)\n",
        "\n",
        "    #     # Bottleneck\n",
        "    #     flops += self.conv.flops()\n",
        "\n",
        "    #     # Decoder\n",
        "    #     flops += self.upsample_0.flops(self.reso//2**4,self.reso//2**4)+self.de_layer0.flops()\n",
        "    #     flops += self.upsample_1.flops(self.reso//2**3,self.reso//2**3)+self.de_layer1.flops()\n",
        "    #     flops += self.upsample_2.flops(self.reso//2**2,self.reso//2**2)+self.de_layer2.flops()\n",
        "    #     flops += self.upsample_3.flops(self.reso//2,self.reso//2)+self.de_layer3.flops()\n",
        "\n",
        "    #     # Output Projection\n",
        "    #     flops += self.output_proj.flops(self.reso,self.reso)\n",
        "    #     return flops\n",
        "\n",
        "\n",
        "input_size = 256\n",
        "depths=[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
        "# model_restoration = Uformer(img_size=input_size, embed_dim=16,depths=depths,\n",
        "#              win_size=8, mlp_ratio=4., token_projection='linear', token_mlp='leff', modulator=True, shift_flag=False)\n",
        "model_restoration = Uformer(in_chans=3, dd_in=3, embed_dim= 16, depths=depths)\n",
        "x = torch.randn(1, 256, 256, 3).permute(0, 3, 1, 2) # first_channel\n",
        "model_restoration(x).shape"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4AKeHReOIAKc",
        "outputId": "310e0d86-2ca6-4f31-8c61-4142e552dc31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}